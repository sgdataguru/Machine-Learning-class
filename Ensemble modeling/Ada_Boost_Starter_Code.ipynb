{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNG3RHvPpn2d"
      },
      "source": [
        "# Implementation of Ada-Boost Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAbwDCCRpn2s"
      },
      "outputs": [],
      "source": [
        "#Importing neccesary packages\n",
        "# Load libraries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR_IsNt-pn2v"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uwMQZMWpn2w"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% training and 20% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u-ACg7Opn2w"
      },
      "outputs": [],
      "source": [
        "# Create adaboost classifer object\n",
        "AdaModel = AdaBoostClassifier(n_estimators=100,\n",
        "                         learning_rate=1)\n",
        "# Train Adaboost Classifer\n",
        "model = AdaModel.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPppbtNZpn2x"
      },
      "source": [
        "#Important Parameters\n",
        "base_estimator: It is a weak learner used to train the model. It uses DecisionTreeClassifier as default weak learner for training purpose. You can also specify different machine learning algorithms.\n",
        "\n",
        "n_estimators: Number of weak learners to train iteratively.\n",
        "\n",
        "learning_rate: It contributes to the weights of weak learners. It uses 1 as a default value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8lv49nKpn2z",
        "outputId": "9019f840-df44-4c90-fe3e-0ec210b59828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D025K-7Gpn21"
      },
      "outputs": [],
      "source": [
        "# Import Support Vector Classifier\n",
        "from sklearn.svm import SVC\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "svc=SVC(probability=True, kernel='linear')\n",
        "\n",
        "# Create adaboost classifer object\n",
        "abc =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1mZ4QB4pn23"
      },
      "outputs": [],
      "source": [
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahc2BvQypn24",
        "outputId": "74ba8dc8-2b3e-4b48-d647-7e25223a44a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDeWIHhpn25"
      },
      "source": [
        "Pros\n",
        "AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. You can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. This can be found out via experiment results, but there is no concrete reason available.\n",
        "\n",
        "Cons\n",
        "AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}